---
date: '2023-12-13'
title: 'ReSkipNet: Skip Connected Convolutional Autoencoder for Original Document Denoising'
type: 'Conference Paper'
conference: 'ICCIT'
doi: '10.1109/ICCIT60459.2023.10441086'
url: ''
featured: false
authors:
  - name: 'Mohammad Muhibur Rahman*'
    affiliation: 'Brac University'
  - name: 'Anushua Ahmed*'
    affiliation: 'Brac University'
  - name: 'Mohammad Rakibul Hasan Mahin'
    affiliation: 'Brac University'
  - name: 'Fahmid Bin Kibria'
    affiliation: 'Brac University'
  - name: 'Waheed Moonwar'
    affiliation: 'Brac University'
  - name: 'Ehsanur Rahman Rhythm'
    url: 'https://errhythm.me'
    affiliation: 'Brac University'
    email: 'errhythm.me@gmail.com'
  - name: 'Annajiat Alim Rasel'
    url: 'http://annajiat.googlepages.com'
    affiliation: 'Brac University'
    email: 'annajiat@gmail.com'
---

Data pre-processing, data analysis, and Optical Character Recognition need a huge amount of clean data, and document images are usually a good source for this. However, document images frequently exhibit blurring and various other forms of noise, which can pose challenges in their manipulation and analysis. To denoise and deblur such document images, autoencoders have been used for a long time. For this task, we propose a novel Convolutional Autoencoder Network which is composed of multiple skip-connected residual blocks and other layers for supporting the encoder and decoder parts. This model not only uses less computational power to denoise existing document image datasets but also performs well. While prior research primarily concentrates on optimizing evaluation metrics, our approach additionally prioritizes larger resolution input sizes. This characteristic of using larger image sizes enhances its practicality and usability as real-world documents are typically characterized by a higher word density. Moreover, in order to further advance the development of our model, we produced an original dataset and proceeded to train our model on this dataset, resulting in satisfactory outcomes.
