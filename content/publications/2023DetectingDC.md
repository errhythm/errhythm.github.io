---
date: '2023-11-23'
title: 'Detecting Derogatory Comments on Women using Transformer-Based Models'
type: 'Conference Paper'
conference: 'COMNETSAT'
doi: '10.1109/COMNETSAT59769.2023.10420692'
url: ''
github: ''
featured: false
authors:
  - name: 'Sara Jerin Prithila'
    affiliation: 'Brac University'
  - name: 'Fariha Hasan Tonima'
    affiliation: 'Brac University'
  - name: 'Tahsina Tajrim Oishi'
    affiliation: 'Brac University'
  - name: 'Md. Nazrul Islam'
    affiliation: 'Brac University'
  - name: 'Ehsanur Rahman Rhythm'
    url: 'https://errhythm.me'
    affiliation: 'Brac University'
    email: 'errhythm.me@gmail.com'
  - name: 'Annajiat Alim Rasel'
    url: 'http://annajiat.googlepages.com'
    affiliation: 'Brac University'
    email: 'annajiat@gmail.com'
---

Natural Language Processing (NLP) is a piqued interest field nowadays, as it helps AI to understand and interpret human languages. In order to facilitate the advancement in this field, in this paper, we propose research on the detection of derogatory comments against women with the help of transformer-based models. Here, our main focus is to detect misogynistic comments, as the women of our country mainly get harassed by such texts. This paper aims to make a comparative study on how efficient transformer models are in detecting gender-biased slandering in languages such as English and Bengali. To carry out this research procedure, the datasets we used were in English and Bengali languages which were further trained across the following transformer models: BanglaBERT, XLM-RoBERTa, m-BERT, and DistilBERT. To give further richness to the paper, the Bengali and English datasets used were created by combining multiple different datasets in these languages. The datasets were extracted from various papers related to this or a similar field of research to help reduce biases and improve language understanding capability. Upon, training our datasets across the mentioned models, for the Bengali dataset, Bangla-BERT-Base performed the best with an F1 score of 94% and for the English dataset, m-BERT scored the best with an F1 score of 86.1%. To add on, since the paper mostly focuses on the Bengali language, it will furthermore, encourage others to increase research on low-resourced languages.
